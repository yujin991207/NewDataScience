{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d953d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d372b31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_header = 'spam\\t'\n",
    "no_spam_header = 'ham\\t'\n",
    "\n",
    "documents = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2368cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SMSSpamCollection','r',encoding = 'utf-8') as file_handle:\n",
    "    for line in file_handle:\n",
    "#         print(line) # 한줄씩 분리\n",
    "        if line.startswith(spam_header):\n",
    "            labels.append(1) # 스팸메일시\n",
    "            documents.append(line[len(spam_header):])\n",
    "        elif line.startswith(no_spam_header):\n",
    "            labels.append(0) # 스팸메일이 아닐시\n",
    "            documents.append(line[len(no_spam_header):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ec2f41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\\n',\n",
       " 'Ok lar... Joking wif u oni...\\n',\n",
       " \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\n\",\n",
       " 'U dun say so early hor... U c already then say...\\n',\n",
       " \"Nah I don't think he goes to usf, he lives around here though\\n\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06e23206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f370499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "term_counts = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdbd1691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '000pes', ..., 'èn', 'ú1', '〨ud'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d1656c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ahhhh', 'ahmad', 'ahold', 'aid', 'aids', 'aig', 'aight', 'ain',\n",
       "       'aint', 'air', 'air1', 'airport', 'airtel', 'aiya', 'aiyah',\n",
       "       'aiyar', 'aiyo', 'ajith', 'ak', 'aka', 'akon', 'al', 'alaikkum',\n",
       "       'alaipayuthe', 'albi', 'album', 'alcohol', 'aldrine', 'alert',\n",
       "       'alertfrom', 'alerts', 'aletter', 'alex', 'alfie', 'algarve',\n",
       "       'algebra', 'algorithms', 'ali', 'alian', 'alibi', 'alive', 'all',\n",
       "       'allah', 'allalo', 'allday', 'alle', 'allo', 'allow', 'allowed',\n",
       "       'allows', 'almost', 'alone', 'along', 'alot', 'already', 'alright',\n",
       "       'alrite', 'also', 'alter', 'alternative', 'although', 'alto18',\n",
       "       'aluable', 'alwa', 'always', 'alwys', 'am', 'amanda', 'amazing',\n",
       "       'ambitious', 'ambrith', 'american', 'ami', 'amigos', 'amk', 'amla',\n",
       "       'amma', 'ammae', 'ammo', 'amnow', 'among', 'amongst', 'amore',\n",
       "       'amount', 'amp', 'amplikater', 'amrca', 'amrita', 'ams', 'amt',\n",
       "       'amused', 'amy', 'an', 'ana', 'anal', 'analysis', 'anand', 'and',\n",
       "       'anderson', 'andre'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[1000:1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59e3129a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8713"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary) # 유니크한단어"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
